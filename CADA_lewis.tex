\documentclass[man, noapacite, 12pt]{apa2}

\makeatletter
\newenvironment{chapquote}[2][2em]
  {\setlength{\@tempdima}{#1}%
   \def\chapquote@author{#2}%
   \parshape 1 \@tempdima \dimexpr\textwidth-2\@tempdima\relax%
   \itshape}
  {\par\normalfont\hfill--\ \chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother

%\usepackage{pdfsync}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{apacite2}
\usepackage{fullpage,rotating}
\usepackage{pslatex}
\usepackage{amssymb}
\usepackage{multirow}


\title{Linguistic structure emerges from cognitive mechanisms}
\author{Molly L. Lewis}
\affiliation{Department of Psychology, Stanford University\\ Conceptual Analysis of Dissertation Area\\ 6 October 2014}

\shorttitle{Linguistic structure emerges from cognitive mechanisms}
\rightheader{Linguistic structure emerges from cognitive mechanisms}
\acknowledgements{Advisor: Michael C. Frank\\ \noindent Additional Readers: Ellen Markman and Noah Goodman}

\abstract{Language is highly structured --- from the way meaning is organized into categories to elaborate regularities in grammar. Where does this structure come from? In this paper, I argue that linguistic structure is causally related to linguistic usage.  In Part I, I begin by arguing that language use is a particular kind of  coordination problem. I suggest that Horn's taxonomy of pragmatic pressures (1984) provides a useful framework for understanding the nature of this coordination problem, in the case of language use. I then highlight a number of phenomena in linguistic use that are reflected in linguistic structure. In Part II, I propose a causal process underlying the similarity between usage and structure. This analysis relies on a division of language into five different timescales: pragmatic, discourse, developmental, cultural, and evolution. I conclude by surveying a range of cognitive phenomena that emerge from the dynamics between these timescales.}


%\abstract{People interact with people and, because of shared interest, try to coordinate their behavior. This behavior is governed by the competition of pragmatic pressures. These pressures lead to equilibria. These equilibria become conventionalized over time (Lewis). This convention becomes an additional pressure in the moment of interaction. Language as a paradigm case of these dynamics.}

\begin{document}
\maketitle

%%%%%%%%% INTRO %%%%%%%%% 
%\begin{chapquote}{Aristotle, \textit{Nic. Ethics II 6}}
%\noindent  Virtue, then, is a state of character concerned with choice, lying in a mean, i.e. the mean relative to us, this being determined by a rational principle, and by that principle by which the man of practical wisdom would determine it. Now it is a mean between two vices, that which depends on excess and that which depends on defect; and again it is a mean because the vices respectively fall short of or exceed what is right in both passions and actions, while virtue both finds and chooses that which is intermediate. Hence in respect of its substance and the definition which states its essence virtue is a mean, with regard to what is best and right an extreme.
%\end{chapquote}

\begin{chapquote}{G.K. Zipf, \textit{1949}}
\noindent  Human society can be viewed as a field which both influences the individual members of the group and is influenced by them. 
\end{chapquote}

\section{Introduction}
``Room for cream?'' asked the barista. ``Mm, yes -- just a bit" replied the customer. Mundane linguistic interactions such as this are the building blocks of daily experience. They are individuals making sounds to each other in an effort to coordinate their behavior in the physical world \cite{clark2006social}. These interactions are messy, variable, and highly unconstrained. Indeed it is this variability that gives language its vast expressive power \cite{hockett1960}. Yet, despite this appearance of irregularity, rich patterns in linguistic usage are revealed when we aggregate across instances of language use both within and across languages. At the level of syntax, for example, there is a strong bias in English to put subjects before verbs and, across languages, this pattern is attested more often than would be expected by chance alone  \cite{dryer2005order}. These types of probabilistic regularities exist at every level of linguistic structure --- from phonology, to semantics, syntax, and discourse --- and researchers from a variety of disciplines have taken as their project the goal of characterizing these regularities.

In this paper, I argue that we can gain  insight into the character of linguistic structure by considering the dynamics of language use. I will suggest the best way to do this is by framing language use as an instance of a broader phenomenon: social interaction \cite{clark1996using}. In particular, I will adopt the formal framework of social interaction proposed by \citeA{schelling1980strategy} in which social interactions are viewed as acts of solving coordination problems.  To illustrate, consider the barista example above. In this example, the agents are the barista and the customer, and they must coordinate how  to fill the coffee mug. There are two outcomes --- full and almost full --- and the barista's desired outcome is determined by the preference of the customer. In this case, the barista and the customer rely on language to coordinate their behavior, but this coordination could have been achieved in other ways (e.g. the customer could have shook her head, pointed to the place inside the mug that she wanted the coffee filled to, etc.). Coordination of their behavior is achieved  by arriving at the mutually preferred outcome (the customer's mug is almost full).

A key tenet to the broader argument is that the act of using language is itself an act of solving a coordination problem \cite{clark1996using}. When a person speaks, there are many possible ways the utterance could be interpreted, and arriving at the intended interpretation is an act of coordination with the listener. For example, in the case of the customer's interaction with the barista, there are many possible interpretations of the phrase, ``Room for cream?." The barista could mean ``Would you like to add cream to your coffee? If so, I will facilitate that by not filling your mug full with coffee." Or, ``We have so much extra inventory of cream! Do you have room in your bag to take some?" Or, ``Do you like the band `Room for cream'?". Or, if the speaker is speaking another language, a totally unrelated meaning. The point is that the speaker's intended meaning is underspecified from the language form alone and the interlocutors must work collaboratively to arrive at a shared understanding. Following \citeA{lewis1969convention}, I will suggest that we can gain insight into the dynamics of linguistic coordination problems by using Schelling's formal framework.  This perspective on language use will ultimately provide a helpful framework for understanding the relationship between language use and language structure.

It is worth reflecting on the historical relationship between these two aspects of language. Across many schools of linguistics, theorists have made a theoretical cut between language use and language structure: {\it parole} vs.\  {\it langue}  \cite{saussure},  {\it token}  vs.\  {\it type}  \cite{peirce}, and  {\it performance}  vs.\  {\it competence}  \cite{chomsky1965aspects}. These theorists  have different views on the ontological status of structure --- Saussure suggests it is a social fact, while Chomsky argues it is fundamentally a cognitive phenomenon --- but they nonetheless agree that there is some sort of invariance in language and it should be the focus of study. Language use has often been seen as an irregular, variant, and epiphenomenal to the true subject of study: structure. However, a number of more recent movements have begun to focus on language use. \nocite{labov197213} Labov's (1972) work was an important challenge to exclusionary focus on abstract structure. His work revealed systematicity in the variation of phonology as function of social variables, suggesting that ``messy" language use was governed by regularities and could therefore be studied scientifically. The study of pragmatics, more generally, can be seen as a step to find regularity in language use. The goal of this paper is to suggest that, not only are these two aspects of language deeply related to each other, but that the key to understanding linguistic structure may lie in understanding linguistic use.

In Part I, I will outline the linguistic coordination problem as a paradigmatic case of the social coordination problem. I will suggest that linguistic coordination problems are solved through the dynamics of two opposing two forces --- the goals of the speaker and the hearer. Following Lewis, I suggest that these opposing forces are resolved by finding an equilibrium point. I will then argue that the equilibria that are reached in language use are reflected in the structure of language, and survey a variety of phenomena in linguistic structure that show this pattern.

In Part II, I will consider the mechanism that might cause linguistic structure to reflect the equilibria reached in linguistic use. I describe five theoretically distinct timescales associated with language, and argue that dynamics between adjacent timescales is responsible for the ultimate emergence of linguistic structure. Given this framework, I will describe a variety of cognitive phenomena that result from the dynamics of these timescales.


%%%%%%%%% PART I %%%%%%%%% 
\section{Part I: Linguistic structure reflects pragmatic equilibria}

Where does linguistic structure come from? \citeA[2010]{christiansen2008} propose a compelling theory. They argue that  multiple cognitive constraints dynamically influence language evolution. They suggest four constraints: the representational format of thought, properties of the percepto-motor system, learning and processing constraints, and constraints that result from reasoning about others' intentions ({\it pragmatic} constraints). Their argument is that these constraints  influence  language at the moment of use, but over time, these biases become instantiated in the structure of language. Although each of these constraints likely plays an important role in the evolution of language, the present paper focuses on the independent contribution of pragmatic constraints. The claim is that pragmatic constraints that play out at the moment of language use become fossilized in the structure of language over time. To develop this claim, we begin by modeling language use as a type of social coordination. We then turn to an analysis of language use as a social coordination problem. Finally, we consider three cases where there are similarities in phenomena between language use and language structure.

\subsection{Social interaction as a coordination problem}
Many theorists of language \cite{zipf1936, lewis1969convention, grice1975logic, clark1996using} have observed that language is an instance of a much broader class of behavioral phenomena --- social coordination. For each, language use is a case of multiple agents making interdependent  rational choices. By adopting work from game theory,  \citeA{lewis1969convention} formalized the notion of language as a coordination problem. He defines a  {\it coordination problem} as follows: \begin{quote} Two or more agents must each choose one of several alternative actions. Often all the agents have the same set of alternative actions, but that is not necessary. The outcomes the agents want to produce or prevent are determined jointly by the actions of all the agents (p. 8).
\end{quote} 
The key feature of these problems is that some combinations of the agents' choices are better than others: there are a set of joint choices in which no agent would have a larger payoff had the agent alone changed her choice. We refer to these as {\it equilibrium points}. 
%footnote{In outlining his seminal theory of pragmatics, Grice  writes: ``As one of my avowed aims is to see talking as a special case or variety of purposive, indeed rational, behavior, it may be worth noting that the specific expectations of presumptions connected with at least some of the foregoing maxims have their analogues in the sphere of transactions that are not talk exchanges" \cite[pg. 47]{grice1975logic}.} 

This broad framing can describe the dynamics of many social interactions. Take the above case of the barista and the customer, for example. We can model this interaction using a payoff matrix (Table 1). In the matrix, we represent the customer's payoff along the rows and the barista's payoff along the columns. There are two possible choices for  level of coffee in the cup---  full and  almost full --- and so each agent gets two rows or columns. 
\begin{table}[t]
\begin{center}
\begin{tabular}{l p{3cm} l p{3cm} l p{3cm} r}
 &  & \multicolumn{2}{c}{customer} \\ \cline{2-4} 
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{full} & \multicolumn{1}{l|}{almost full} \\ \cline{2-4} 
\multicolumn{1}{c|}{\multirow{2}{*}{barista}} & \multicolumn{1}{l|}{full} & \multicolumn{1}{l|}{0,0} & \multicolumn{1}{l|}{0,0} \\ \cline{2-4} 
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{almost full} & \multicolumn{1}{l|}{0,0} & \multicolumn{1}{l|}{1,1} \\ \cline{2-4} 
\end{tabular}
\caption{A payoff matrix of a simple interaction --- a barista trying to determine how full to fill a customers' cup. Given the customer's preference for cream, there lies an equilibrium point at `almost full' for both the barista and the customer. To arrive at this equilibrium point, the two must coordinate.}
\end{center}
\end{table}
The agents relative payoffs are indicated in the cells, with the barista's on the left, and the customer's on the right. This happens to be a very simple equilibria --- there is one, and only one, possible set of actions in which is an equilibrium. The customer prefers almost full and the barista fills the cup to almost full. The problem is that the barista does not know a priori where this equilibrium lies, i.e. that the customer's pay off for almost full is 1, relative to 0 for full. To solve this coordination problem, the customer and barista make use of language.

More complicated coordination problems arise when there are multiple possible equilibria. Consider a weekend trip in which food and alcohol must be brought. To distribute the burden, half of the vacationers will bring food and the other half alcohol. In this case, the payoff matrix might look something like Table 2. Neither group --- Group A or B --- has a strong preference about which of the two commodities each brings. However, what is important is that one group brings food and the other alcohol (no one will be happy on a weekend trip with only food or only alcohol). There are thus two equilibria, one  at each set of choices where the two groups bring different things. By chance, the vacationers are equally likely to end up in a non-equilibrium as they are an equilibria. They must  therefore coordinate, via language or some other means, to ensure that they end up at an equilibrium.

\begin{table}[t]
\begin{center}
\begin{tabular}{l p{3cm} l p{3cm} l p{3cm} l}
 &  & \multicolumn{2}{c}{Group A} \\ \cline{2-4} 
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{food} & \multicolumn{1}{l|}{alcohol} \\ \cline{2-4} 
\multicolumn{1}{c|}{\multirow{2}{*}{Group B}} & \multicolumn{1}{l|}{food} & \multicolumn{1}{l|}{0,0} & \multicolumn{1}{l|}{1,1} \\ \cline{2-4} 
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{alcohol} & \multicolumn{1}{l|}{1,1} & \multicolumn{1}{l|}{0,0} \\ \cline{2-4} 
\end{tabular}
\caption{A payoff matrix for a social interaction in which there are two equilibria. Neither group cares who brings what commodity on the vacation; they only care that they bring different things. }
\end{center}
\end{table}

What are the psychological forces that support these coordination games? Zipf's theory of human behavior (1949) provides  insight. He argued that all human behavior could be accounted for by a single principle: people are motivated to minimize effort ({\it The Principle of Least Effort}). To understand this principle, consider a context in which an individual needs to exert some physical effort, say in walking to a park. The principle predicts they should be motivated to find the solution that minimizes how much effort required (i.e., by finding the shortest path). Critically, Zipf argued this simple principle had explanatory power at the level of social groups. He claimed that this principle operates at the level of the individual, but with interaction, this principle leads to an equilibrium in behavior at the level of the group.

This simple theory of behavior provides a parsimonious account of a wide range of social phenomena. A particularly clear example is the organization of people into social  groups in physical space \cite{zipf1949human}.\footnote{A second, well-studied example is economics. At the level of the individual, the consumer tries to minimize something of value, but in this case the valued commodity is money, rather than physical energy (though these are arguably related in important ways). This is the study of microeconomics. With interaction among consumers, this single force leads to regularities at the level of the social group, or the economy--- the study of macroeconomics.} Zipf assumes that every individual in a society is both a consumer and producer of goods. Governed by the Principle of Least Effort, the individual should minimize effort in terms of movement across land, by consuming (i.e.\ living) and producing (i.e.\ working) at the same location. However, as the number of raw goods increases this becomes increasingly difficult because the consumer cannot not live at the doorstop of every finishing plant. This creates a conflict. On the one hand, there is a tendency to diversify, so that the population lives at the doorstep of the production line. This creates a pressure for many physically separated communities, each producing a single good, but with little trade between communities. On the other hand, there is force to unify so that it is easier to trade final goods. The net result is an equilibrium where people live in many different urban centers across the land. This general theory is reflected in more modern theories of urban spatial layouts \cite{mills1967aggregative, brueckner1987structure}.

Importantly, Zipf's principle does not provide a description of how individuals come to solve pure coordination problems like the vacationer example above. In that case, it is not clear how two individuals with the exact same payoff structure would arrive at the same, otherwise arbitrary solution. To address this issue, we need the idea of {\it convention} which will return to in Part II. However, Zipf's principle does provide insight into how a single psychological force, shared by all individuals, can lead to biases for different alternatives (e.g., a preference for the shortest path to a location). When individuals with these same biases interact with each other, we see the emergence of an equilibrium. 

\subsection{Language use as a coordination problem}
The coordination  framework can be straight-forwardly applied to language use \cite{lewis1969convention}. Language use is a paradigmatic case of a coordination problem because it is a tool that is universal in a community, easy to use, and capable of expressing complex ideas. In the case of language, the core of the coordination problem lies in the resolution of reference. Broadly, resolving reference requires interpreting a meaning from some utterance in a particular context. At the level of individual lexical items, this is a difficult problem because the relationship between linguistic form and meaning is arbitrary \cite{saussure, hockett1960}. That is, knowing the form of a word does not give a listener any insight into the meaning of that word. Consequently, speakers must coordinate their behavior in order to successfully refer.

%\begin{table}[t]
%\begin{center}
%\begin{tabular}{l p{3cm} l p{3cm} l p{3cm} l}
% &  & \multicolumn{2}{c}{Speaker 1} \\ \cline{2-4} 
%\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{``fep"} & \multicolumn{1}{l|}{``dax"} \\ \cline{2-4} 
%\multicolumn{1}{c|}{\multirow{2}{*}{Speaker 2}} & \multicolumn{1}{l|}{Object A} & \multicolumn{1}{l|}{0,0} & \multicolumn{1}{l|}{1,1} \\ \cline{2-4} 
%\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{Object B} & \multicolumn{1}{l|}{1,1} & \multicolumn{1}{l|}{0,0} \\ \cline{2-4} 
%\end{tabular}
%\caption{A payoff matrix of the mapping problem. Given two words and two referents, the mappings are arbitrary. The only constraint is that no word should map to more than one object, and no object should map to more than one word. Thus, as in the vacationer example, speakers must coordinate.}
%\end{center}
%\end{table}

\begin{table}[t]
\begin{center}
\begin{tabular}{l p{3cm} l p{3cm} l p{3cm} l}
 &  & \multicolumn{2}{c}{Speaker} \\ \cline{2-4} 
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\[ 
\left \{
  \begin{tabular}{ccc}
  Object A--``dax" \\
  Object B--``fep"
  \end{tabular}
\right \}
\]} & \multicolumn{1}{l|}{\(
\left \{
  \begin{tabular}{ccc}
  Object A--``fep" \\
  Object B--``dax"
  \end{tabular}
\right \}
\)} \\ \cline{2-4} 
\multicolumn{1}{c|}{\multirow{2}{*}{Listener}} & \multicolumn{1}{l|}{\[ 
\left \{
  \begin{tabular}{ccc}
  Object A--``fep" \\
  Object B--``dax"
  \end{tabular}
\right \}
\]} & \multicolumn{1}{l|}{0,0} & \multicolumn{1}{l|}{1,1} \\ \cline{2-4} 
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\[ 
\left \{
  \begin{tabular}{ccc}
  Object A--``dax" \\
  Object B--``fep"
  \end{tabular}
\right \}
\]} & \multicolumn{1}{l|}{1,1} & \multicolumn{1}{l|}{0,0} \\ \cline{2-4} 
\end{tabular}
\caption{A payoff matrix of the mapping problem. Given two words and two referents, the mappings are arbitrary. The only constraint is that no word should map to more than one object, and no object should map to more than one word. Thus, as in the vacationer example, speakers must coordinate.}
\end{center}
\end{table}

The arbitrariness of linguistic form leads to a formal equivalence between the problem of reference and the problems of coordination described above.  To understand this similarity, consider  a case where  there are two novel words, ``dax" and ``fep," and two novel objects, Object A and Object B. Given this information alone, the listener has no a priori insight into which object each word refers to. This is a problem because the two interlocutors must somehow arrive at the same mappings between words and referents in order to communicate (a system in which you call Object A ``fep" and I call it ``dax" is a terrible communication system). The interlocutors must therefore coordinate. 

The payoff structure for this problem is identical to the vacationer example above (Table 3). Because language is arbitrary, neither speaker cares whether you call Object A ``fep" or ``dax;" they only care that their mappings are the same.  These general dynamics are true not only of individual lexical items, but of all cases of reference. Consider again our example of the barista and the customer. In interpreting the phrase, ``Room for cream?," the individual lexical items are relatively unambiguous --- presumably both know what ``room" and ``cream" mean. But, the intended meaning of the entire phrase is underspecified, and so the interlocutors must work together to resolve meaning.

In this framework, we can think of {\it pragmatics} as the study of the psychological processes that lead to an equilibrium in these referential coordination problems. Pragmatics, then, is just a specific case of the dynamics described by \citeA{zipf1949human}. In the case of language, Zipf's insight was that speech could be thought of as its own economy, similar to any other social system. Speech has a physical cost and could be used as tool. He suggested the speaker and the listener were both governed by the Principle of Least Effort and this lead to an equilibrium. In the case of the speaker, effort could be minimized if there existed a single word {\it w} that could be used to refer to the set of all concepts {\it C}. In the case of the listener, effort would be minimized (in terms of understanding) if there existed a unique word for each  unique concept {\it c}. The dynamics of the interaction of these two opposing forces is pragmatics. 
%synthesis of two antitheses (horn, 22)
Many theorists have tried to account for pragmatic regularities in behavior, most notably \citeA{grice1975logic}. However, \citeA{horn1984} presents a particularly parsimonious theory that closely aligns with Zipf's more general formulation. He posited two principles that describe the manifestation of Zipf's Principle of Least Effort for the speaker and the hearer each. 
\begin{quote} \textsc{speaker}: Say no more than you must. ({\it Principle of Necessity}) \end{quote}
\begin{quote} \textsc{hearer}: Say as much as you can. ({\it Principle of Sufficiency})\footnote{Horn refers to these as the {\it R} and {\it Q} principles, respectively. I've opted  for less opaque terminology.} \end{quote}
These principles are quite straight forward. As a speaker with a necessary meaning to contribute, you want to say a little as possible, while still conveying the intended meaning. In contrast, as a listener, you want the speaker to say as much as possible to minimize your effort at arriving at the correct interpretation. That is, you want the speaker to use sufficient language. In the limit, each strategy on its own does not result in a successful communication system. This is because the speaker's and the hearer's goal are fundamentally linked: While in the short term, in might be less effort for the speaker to utter a single sound, ``blah," to convey her meaning, this will lead to confusion on the part of the hearer, which the speaker will then have to clarify with additional language. The speaker and hearer must therefore resolve their two opposing forces  in what Horn called a ``division of pragmatic labor"  in order to arrive at an equilibrium point \cite[p. 22]{horn1984}.\footnote{Note that there is a super maxim that is also operating here: the cooperative principle \cite{horn1984, grice1975logic}. The cooperative principle is essentially the idea that the interlocutors realize that utterances are the result of an equilibrium from the dynamics of these two forces. Put another way, it is a statement that the interlocutors realize that they are playing a coordination game.}

\begin{figure}
\begin{center} 
\includegraphics[width=3in]{figs/frank2012.png}
\caption{\label{fig:frank2012}  Example stimuli from Frank and Goodman (2012). In this case, the intended referent is the middle shape. Both speaker and hearer agree that the utterance ``circle" is  an equilibrium point, as compared to ``blue," even though both terms equally describe the intended referent in truth-functional terms.}
\end{center} 
\end{figure}

While these principles are simple, the dynamics they give rise to are complex. \citeA{frank2012predicting} present a formal model that captures these dynamics in a simple reference game. In their game, there are three possible referents and each referent has two relevant features (Fig.\ 1). Given the constraint that a speaker can only utter a single word, there are always two possible words the speaker could utter. For example, if the  intended referent is a blue circle, a speaker could use either the word ``blue" or ``circle" to refer to the object. Critically, both words are equally true of the referent from the perspective of truth functional semantics. The phenomenon that this model captures is that the speakers use different words to refer to an object --- and that listeners expect them to --- depending on the context in which the word is uttered. In particular, speakers  tend to choose a word that most uniquely identifies the intended referent, given the referential context. In other words, they select the word that is most informative. For example, in the example trial pictured in Figure 1, speakers tend to use the word ``circle" instead of ``blue'' to identify the middle referent.  Frank and Goodman use a Bayesian framework to formalize this notion of informativity, and their model closely captures the behavioral data in this reference game. This suggests that the behavior of both interlocutors is guided by a tacit understanding of informativity in the referential context. 

This notion of informativity falls directly out of Horn's principles. In this reference game, speakers are constrained by the length of the utterance they can use (one word), but the choice of words is free. From the listener's perspective, the utterance must be sufficient and so uttering ``blue" would be insufficient in the above context because it is ambiguous, and there is a better alternative. From the speaker's perspective, the utterance must be necessary, but should not be too verbose. This force is enforced by structure of the task --- the speaker must contribute something to participate in the task, and the utterance cannot be overly verbose given the single word constraint. Maximal informativity --- a sufficient contribution, given the context --- is thus the equilibrium between these two forces.

\subsection{Pragmatic equilibria reflected in the structure of language}

Simple reference games like that of \citeA{frank2012predicting} are one example of the kind of equilibria that emerges from the interaction of Horn's principles, but there are many others. In the present section, we consider how these  pragmatic equilibria points in language use are reflected in the structure of language. We will consider this relationship for three different kinds of linguistic structure: semantics, words, and syntax.

\subsubsection{Semantics}
Semantics concerns the context-independent meaning associated with a word. The size of the semantic space denoted by a particular word reflects an equilibrium point between Horn's speaker and hearer principles.  From the hearer's perspective, Horn argues there is a pressure  to narrow semantic space \cite{horn1984}. This reflects the idea that the hearer's optimal language is one in which every possible meaning receives its own word. One example of this is the word ``rectangle." This word refers to a quadrilateral with four right angles. A special case of a ``rectangle"  is a case where the four sides are equal in length, which has its own special name, ``square." Consequently, the term ``rectangle" has been narrowed to mean a quadrilateral with four right angles, where the four sides are {\it not} equal.\footnote{Horn also points out that there are cases of narrowing that are speaker-based, as in ``drink" for ``alcoholic drink."} From the speaker's perspective, there is a pressure for semantic broadening. This is because the speaker's ideal language is one in which a single word can refer to a wide range of meanings. An example of this is the broadening of brand names to refer to a kind of product. For example, ``kleenex" is a name of a product name for facial tissues, but has taken on the meaning of facial tissues more generally.

The opposition of these two semantic forces predicts an equilibrium in the organization of semantic space that satisfies the pressures of both speaker and hearer. A body of empirical work has tested this prediction by examining the organization of particular semantic domains cross-linguistically \cite{regierword}. Languages show a large degree of similarity in how they partition semantic space for a particular domain, which is likely due to universal cognitive constraints. But, they also show a large degree of variability and these different systems can be shown to all approximate an equilibrium point between speaker and hearer pressures. 

 \citeA{kemp2012kinship} demonstrate this systematicity in the semantic domain of kinship. For each language, they developed a metric of the degree to which  Horn's speaker and hearer pressures (in their terminology: communicative cost and complexity, respectively) are satisfied. A language that better satisfies the hearer's pressure is one that is more complex, as measured by the length of the description of the system in their representational language. A language that better satisfies the speaker's pressure is one that requires less language to describe the intended referent. To understand this, consider the word ``grandmother" in English: this word is ambiguous in English because it could refer to either the maternal or paternal mother, and so identifying one in particular is more costly in English than in a language that encodes this distinction lexically. They find that the set of attested languages is a subset of the range of possible languages, and this subset partitions the  semantic space in a way that is near the optimal tradeoff between speaker and hearer pressures (Fig.\ 2). This type of analysis has also been done for the domains of color \cite{regier2007color}, light \cite{baddeley2009relationship}, and numerosity \cite{xu4numeral}.
 
 \begin{figure}
\begin{center} 
\includegraphics[width=2in]{figs/kemp2012.pdf}
\caption{\label{fig:frank2012} Plot  from Kemp and Regier (2012). Languages organize the semantic space of kinship in a way that optimizes both speaker and hearer pressures. The notion of {\it communicative cost} maps onto Horn's speaker principle and the notion of {\it complexity} maps onto Horn's hearer principle. Gray circles represent possible systems and black circles represented actually attested languages. The key observation is that all of the attested languages are clustered around the bottom left corner that corresponds to an equilibrium between speaker and hearer pressures.} 
\end{center} 
\end{figure}

A second phenomenon that is predicted by these forces is the presence of lexical ambiguity. That is, cases in which there are multiple meanings associated with a word, from a context-independent perspective. Language is rampant with examples. For example, the word ``bat" could mean either the instrument used in baseball or the flying mammal. This type of ambiguity in context-independent meaning is tolerated because the meaning is usually easily disambiguated by context. When the word ``bat" is uttered while watching a baseball game,  the mammal usage of the word is very unlikely. We can view the presence of this ambiguity as an equilibrium in Horn's speaker and hearer principles. If the meaning of a word can be disambiguated by the referential context, then it would violate the speaker's principle to have an overly-specific term for a meaning. 

Indeed, recent work by \citeA{piantadosi2011b} reveals systematicity in the presence of lexical ambiguity in language. They argue that ambiguity results from a speaker based pressure to broaden the meaning of a word to include multiple possible meanings. In particular, they suggest that this pressure should lead to a systematic relationship between the presence of ambiguity and the cost of a word. According to their argument, costly words (in terms of length, frequency, or any metric of cost) that are easily understood by context violate the speaker's principle to say no more than you must. Consequently, there should be a pressure for these meanings to get mapped on to a different, less costly word. This word may happen to already have a meaning associated with it, and so the result  is multiple meanings being mapped to a single word. For example, in the case of the word ``bat," a speaker could instead say ``baseball bat." But, because this referent is easily disambiguated in context from the mammalian meaning,  Horn's speaker principle leads to a pressure to use the shorter form.\footnote{There are also cases of temporary ambiguity in the meanings of syntactic structures. For example, in a sentence that begins ``The coach knew you..." it is unclear whether ``you" is a direct object or the subject of a relative clause. Speakers can avoid this ambiguity by inserting a ``that," but this is optional. Work by \citeA{ferreira2000effect} suggests that speakers often do not avoid this ambiguity, presumably because the intended meaning can easily be recovered from context.} This leads to a testable prediction that shorter words should tend to be more ambiguous.  Through corpus analyses, \citeA{piantadosi2011b} find this precise  relationship between cost and ambiguity. They find a linear relationship between word length and ambiguity across English, Dutch and German: Shorter words are more likely to have multiple meanings. 

An additional case of this lexical ambiguity is found in words that have very little context-independent meaning, known as indexicals or deictics \cite{frawley2003international}. These words get their meaning from the particular referential context of the utterance, and are therefore highly ambiguous from a context-independent perspective. There are many types of indexicals that are present to varying degrees across languages. An example of a temporal indexical form is ``tomorrow." The context-independent meaning of this word is something like ``the day after the day this word is being uttered in.'' Critically, abstracted from any context, this word has little meaning; it is impossible to interpret without having knowledge about the day the word was uttered. This phenomenon is also present in person pronouns (e.g.\ ``you" and ``I") and spatial forms, like ``here" and ``there."  As for lexical ambiguity, this type of ambiguity is a predicted equilibrium point from Horn's principles: If the hearer can recover the intended referent from context, the speaker would be saying more than is necessary by using an overly-specific referential term (e.g., ``December 18th, 2014" vs.``tomorrow"). Language structure reflects this pressure through lexicalized ambiguity in the form of indexicals.

\begin{figure}
\begin{center} 
\includegraphics[width=3in]{figs/onetoone.pdf}
\caption{\label{fig:frank2012} Three possible structures on the organization of lexicon. According to Horn's principles, the hearer's optimal language is one in which there is a one-to-one mapping between words and concepts. The speaker's optimal language is one in which there is one word that maps to many concepts. Synonymy, or a many-1 structure, is thus dispreferred by both interlocutors.}
\end{center} 
\end{figure}

Finally, the relationship between the meanings of different words can be seen as a consequence of Horn's principles. A number of theorists have noted a bias against two words mapping onto the same meaning --- that is, a bias against synonymy \cite{saussure, kiparsky1983word, horn1984, clark1987principle, clark1988logic}. This bias is an equilibrium between Horn's speaker and hearer principles. Recall that the optimal language for a hearer is one in which each meaning maps to its own word --- exactly a language biased against synonymy (see Fig. 3). It turns out that the speaker's pressure also biases against synonymy.  The optimal language for the speaker is a language where a single word maps to all meanings. But, a case where multiple words map to a single meaning is also undesirable because the speaker must keep track of two words. So, for both the speaker and the hearer, there is pressure to avoid synonymy. Thus, when a listener hears a speaker use a second word for an existing meaning, the hearer infers that this could not be what the speaker intended because this would violate the speaker's principle. The result is an assumption that the second word maps to a different meaning. This pattern is reflected in language structure by a one-to-one pattern in the lexicon --- that is, a structure in which each word maps to exactly one meaning and each meaning maps to exactly one word.

As one kind of evidence for this one-to-one structure in the lexicon, \citeA{horn1984} points to a phenomenon called {\it blocking}. Blocking refers to cases in which an existing lexical form blocks the presence of a different, derived form with the same root. Consider the following examples:
 \begin{quote} 
 	(a) fury furious *furiosity\\
	(b) *cury curious curiosity 
\end{quote}
In both (a) and (b), forms that would be expected, given the inflectional morphology in English, are not permitted. This is presumably because they would have the same meaning as the existing form because they have the same root. Examples such as this provide some evidence for a one-to-one structure in language, but a one-to-one structure is a particularly difficult linguistic regularity to test empirically. Nonetheless, it is an important regularity because it licenses certain inferences in interpreting the meaning of words. In particular, the cognitive representation of a one-to-one regularity has been posited as an explanation of children's bias to map a novel word onto a novel object \cite{markman1988, markman2003}. We return to this issue in Part II.

\subsubsection{Words}
Horn's principles make a prediction about the relationship between the length of utterances and their meanings. In many cases, it is possible to use two different utterances to refer to the same meaning (in truth functional terms), and often these utterances differ in length. \citeA{horn1984} presents the following example: 
\begin{quote} 
 	(1a) Lee stopped the car.\\
	(1b) Lee got the car to stop.
	
	%(2a) Black Bart killed the sherif.\\
	%(2b) Black Bart caused the sherif to die.
\end{quote}
Both (a) and (b) have the same denotational meaning (the successful stopping of a car), but they differ in length ((b) has two extra words). Horn argues that this asymmetry leads to an inference on the part of the listener that the two differ in meaning. The logic of this inference is  identical to the lexical structure case above. The listener hears a speaker use a more costly phrase to express a meaning that could have been expressed in a less costly way. The listener thus infers that this other meaning could not be what the speaker intended because this would violate the speaker's principle to say no more than is necessary. Horn adds an additional layer to this argument. He suggests that no only do these two forms differ in meaning, but that they map onto meanings in a systematic way. In particular, he argues that the longer form gets mapped on to the more marked meaning, while the shorter form refers to the unmarked meaning.  The notion of `markedness' is underspecified here, but an intuitive definition is related to complexity: more marked things are more conceptually complex, while less marked things are more conceptual simple.  Thus, in the above example, (a) would refer to a simple, average case of car stopping, while (b) might refer to case where something complex or unusual happened, perhaps because Lee used the emergency brake.

\begin{table}[t]
\begin{center}
\begin{tabular}{l p{3cm} l p{3cm} l p{3cm} l}
 &  & \multicolumn{2}{c}{Speaker} \\ \cline{2-4} 
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\[ 
\left \{
  \begin{tabular}{ccc}
  short--simple \\
  long--complex
  \end{tabular}
\right \}
\]} & \multicolumn{1}{l|}{\(
\left \{
  \begin{tabular}{ccc}
  short--complex \\
  long--simple 
  \end{tabular}
\right \}
\)} \\ \cline{2-4} 
\multicolumn{1}{c|}{\multirow{2}{*}{Listener}} & \multicolumn{1}{l|}{\[ 
\left \{
  \begin{tabular}{ccc}
  short--complex \\
  long--simple
  \end{tabular}
\right \}
\]} & \multicolumn{1}{l|}{0,0} & \multicolumn{1}{l|}{1,1} \\ \cline{2-4} 
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\[ 
\left \{
  \begin{tabular}{ccc}
  short--simple \\
  long--complex 
  \end{tabular}
\right \}
\]} & \multicolumn{1}{l|}{1,1} & \multicolumn{1}{l|}{0,0} \\ \cline{2-4} 
\end{tabular}
\caption{The payoff matrix for the speaker and listener in solving a coordination problem in which there are two words --- one short and one long --- and two meanings --- one simple and one complex. Given these constraints, there are two equilibrium lexicons. As observed by Horn (1984), speakers tend to arrive at the equilibrium in the bottom left corner. }
\end{center}
\end{table}

The source of the particular mapping between forms of different lengths and meanings of different degrees of markedness is unclear. This is because, in principle, there are multiple equilibrium points in the mapping between form and meaning. Assuming a one-to-one constraint on the mapping, there are two possible equilibria: \{short--simple, long--complex\} or \{short--complex, long--simple\} (Table 4). Both satisfy the constraint that each  form gets mapped to a unique meaning. So how do speakers arrive at the  \{short--simple, long--complex\} equilibrium? This is a difficult result to derive from models of pragmatic reasoning. \citeA{bergen2014} successfully derive this result as a consequence of the fact that \{short--simple, long--complex\} is a more optimal mapping for the speaker (the indirect result of Zipf's Principle of Least Effort). Another possibility relies on iconicity: hearers have a cognitive bias to map more complex sounding forms to meanings that are similarly complex. 

Despite the absence of clear theoretical account of this phenomenon, the empirical data suggest that learners do indeed arrive at the predicted equilibrium. \citeA{bergen2012} provide evidence for this type of implicature in a communication game.  In their task, partners were told that they were in an alien world with three objects and three possible utterances of different monetary costs. They operationalize the idea of markedness or complexity as frequency, such that participants were instructed that each of the three different objects had three different base rate frequencies  associated with them.  Participants' task was to communicate about one of the objects using one of the available utterances. If they successfully communicated, they received a reward. The results suggest that both the speaker and hearer expected costlier forms to refer to less frequent meanings. This study provides one data point suggesting that Horn's predicted equilibrium between word length and meaning emerges in coordination games.  

There is a growing body of evidence suggesting this equilibrium is also reflected in the structure of words. One approach to testing this hypothesis is to use the linguistic context of a word to measure the complexity of meaning. The idea is that words that are highly predictable, given the linguistic context, have more complex meanings, while words that are less predictable given the linguistic context, have less complex meanings. \citeA{piantadosi2011a} measured  the relationship between the predictability of words in context and the length of words. Across 10 languages, these two measures were highly correlated: words that were longer were less predictable in their linguistic context on average. This result held true even controlling for the frequency of words. Additional evidence for this relationship comes from examining pairs of words that  have  very similar meaning, but differ in length \cite<e.g. ``exam" vs.\ ``examination;">{mahowald2012info}. Through corpus analyses, they find that the longer forms are used in less predicable linguistic contexts. In a behavioral experiment, they also find that  speakers are more likely to select the longer word in unsupportive contexts. This body of work points to a systematic relationship between word length and meaning, when complexity is operationalized as predictability in the linguistic context.

Some of our own work provides more direct evidence for this equilibrium. Given a novel word, we find that both adults and preschoolers are more likely to map a longer word to a more complex object, as compared to a short word \cite{lewis2014structure}. A key difference between our work from prior work is that we  directly manipulate the complexity of word meaning, rather than using the predictability of linguistic context as a proxy. We have operationalized complexity in three different ways. The first is to directly manipulate the number of object parts the referent has. Second, we have measured complexity by obtaining complexity norms from participants on real objects. Third, we have operationalized complexity through a reaction time measure. In each case, we see a bias to map longer words to more complex referents, as compared to a short word. We also find this bias in natural language. We asked participants to rate the complexity of the meaning  of 499 English words, and found that these ratings were highly correlated with word length in both English and 79 other languages. Taken together, this work provides strong evidence that the equilibrium between word length and complexity of meaning found in coordination games, such as \citeA{bergen2012}, is also reflected in the structure of the lexicon. 

\subsubsection{Syntax}
The order of words, or syntax, is another level of language structure that reflects equilibria of language use. \citeA{levinson2000presumptive} provides a detailed account of how Chomsky's syntactic binding constraints can be reinterpreted as pragmatic equilibria. Chomsky argues there are three different principles that govern how pronouns can be co-indexed with their antecedents. I will highlight two here. The first principle is that an anaphor (like ``herself") is bound in its governing category (e.g., a sentence). The second is that a pronoun cannot be co-indexed with a c-commanding\footnote{The term {\it c-command} refers to a specific relationship in generative grammar which is not relevant here. Broadly, it suffices to say that subjects c-command objects in English.} noun phrase.  This constraint provides an of account why (a) below is grammatical, but (b) is not.
 \begin{quote} 
 	(a) $Elsa_1$ $likes$ $herself_1.$\\
 	(b) *$Elsa_1$ $likes$ $her_1.$
\end{quote}
In (a), the pronoun ``herself" is an anaphor and thus is grammatical by Chomsky's first principle. In contrast, in (b), ``her" is co-indexed with a c-commanding noun phrase (``Elsa"), and thus is ungrammatical. Levinson offers an alternative explanation based on pragmatics. The explanation relies on the same logic that leads speakers to avoid synonymy. The logic can be informally summarized as follows:
 \begin{quote} 
 	1. ``herself" is an anaphor and is therefore co-indexed with a noun phrase in the local domain. (identical to Chomsky's first principle, but could also be motivated pragmatically)\\
	2. In (b), the speaker used a different form (``her").\\
	3. If the speaker had meant to refer to the antecedent in the local domain, she would have used ``herself" because it is more informative. (by the hearer principle)\\
	4. The speaker did not, and thus the intended interpretation must be an antecedent outside the local domain.  (i.e., $Elsa_1$ $likes$ $her_2.$)
\end{quote}
This account, which relies only on general principles of pragmatics, is able to account for the observed pattern of grammaticality judgments.
	
Several experimental findings  also  suggest that there are pragmatic equilibria reflected in syntax. One of the primary patterns of linguistic structure  to be explained is the linear structure of words. In particular, why some word orders are much more prevalent across languages than others. \citeA{gibson2013noisy} offer an account of one aspect of this regularity and this account can be interpreted as a suggestion that a pragmatic equilibrium is reflected in the structure of language. 

Their argument is as follows. There is some evidence that subject-object-verb (SOV) word order is the cognitive default \cite<e.g.>{senghas2004children}. This proposal reflects the fact that SOV order is the most prevalent word order cross-linguistically (47\%). But a puzzle still remains: If there is a SOV bias, why is a second order --- SVO --- almost equally as prevalent (41\%)? Gibson et al.\ (2013) propose that the move from an SOV to SVO word order reflects a communicative pressure. The idea is that subjects and objects are often semantically confusable (because they are both entities), and thus, it is possible in  the ``noisy channel" of communication for one of the noun phrases to get deleted. This leads to confusion on the part of the hearer. Thus, they argue that the two  should be linearly separated  with the verb argument in order to avoid confusion. This is advantageous because if an argument is deleted, the correct grammatical relation of the communicated argument can be inferred from the input in an SVO order, but not an SOV order. 

Though they motivate this prediction from an information theoretic perspective, this result can also be derived from Horn's principles. Both the hearer and the speaker want to minimize effort. If it is indeed the case that SOV order frequently leads to confusion on the part of the hearer, this becomes problematic for the speaker who will have to often expend extra effort to clarify the confusion. Thus, given that there is an alternative word order that requires the same amount of effort --- SVO order ---  there should be a pressure on the part of the speaker to move towards using this order.

To test this proposal, Gibson et al.\ (2013) asked speakers to produce sentences describing scenes. A critical prediction of their argument is that confusion should be more likely when both arguments are animate (e.g.\ ``The girl pushed the boy") compared to a case when there is an asymmetry in animacy (e.g.\ ``The girl pushed the car"). In their key experiment, speakers of Japanese and Korean viewed brief videos of events. Japanese and Korean speakers were selected because their native languages were not SVO. Following each event, participants used non-verbal  gesture to represent the events. Critically, the events  involved three nouns that required using and embedded clause structure to describe (e.g., ``The woman says the fireman kicked the girl.").  The patient of the embedded clause was either animate or inanimate. The measure of interest was the location of the embedded clause as a function of the confusability of the  noun phrases. If participants gesture in accord with their native word order, they should show a SOV ($S_{1}\  [S_{2}\ O_{2}\ V_{2}]\  V_{1}$) pattern in their gestures. However, if this bias is sensitive to the confusability of  referents, they should be more likely to show a SVO pattern ($S_{1}\  V_{1\ }[S_{2}\ O_{2}\ V_{2}])$ when all the noun phrases are animate. Consistent with the prediction, this is exactly what they found: Japanese and Korean speakers were more likely to use the SVO pattern when the entities were all animate, as compared to when the embedded patient was inanimate.

Case marking is a second case in which pragmatic equilibria are reflected in syntax. In many languages, case marking is used as a syntactic strategy to indicate  grammatical relations, like subject or object. Case markers are affixes that attach to the noun root, and are typically used in languages where word order is not a cue to grammatical relations. Case marking is usually optional but principled in its usage based on semantic properties of the nouns like animacy. For example, speakers often case mark nouns that appear in unpredictable roles, such as when an inanimate noun functions as a subject. Following logic similar to  \citeA{gibson2013noisy},  \citeA{fedzechkina2012language} used an artificial learning paradigm to test whether speakers regularize a language to minimize confusion between nouns. They predicted that speakers should be more likely to use case marking when the nouns were all animate, and thus confusable. They exposed learners to a verb-final language with flexible constituent order and optional case-marking. Critically, case-marking in the input language was not conditioned on animacy. Consistent with the prediction, they found that learners tended to mark animate objects with an overt case marker more frequently than inanimate objects (because animate objects are confusable  with animate subjects). This provides another case in which pragmatic pressures are reflected in  regularities in language structure.

 
%%%%%%%%% PART II %%%%%%%%% 
\section{Part II: A casual link between linguistic use and linguistic structure}
Why does language structure reflect  patterns of language use? In the present section, I propose a speculative answer to this question. The answer  posits an indirect causal link between language use and language structure: Language use over time leads to regularities in linguistic structure. I will outline this answer by relying on an analysis of language separated by five different timescales. To preview, the hypothesis is that linguistic structure reflects language use as a consequence of local dynamics between adjacent timescales.

A {\it timescale} is a unit of time over which significant changes in state occur. For example, the timescale of dinner is about an hour, where the significant changes in state are walking to the restaurant, sitting down, ordering food, eating the meal, then dessert, etc. In contrast, the timescale of gaining weight occurs over weeks, where the significant changes in state correspond to appreciable weight changes. Critically, the length of the timescales is determined by the change of interest. 

\begin{figure}
\begin{center} 
\includegraphics[width=6in]{figs/timescales1}
\caption{The five linguistic timescales. Each timescale is nested within a slice of longer timescales. The claim is that the dynamics between adjacent timescales (e.g., pragmatic and discourse) lead to changes over time in longer timescales. The developmental timescale is characterized by a particular person from a particular generation, $p_A$, interacting with a series of people over the lifespan. Repeated interactions with the same person, $p_B$, characterize the discourse timescale.}
\end{center} 
\end{figure}

In the case of language,  there are five timescales over which significant changes occur (Fig. 4). The first is the {\it pragmatic timescale}. The pragmatic timescale corresponds to the processes described by Horn's principles, and addressed in detail in Part I. The {\it discourse timescale} is a slightly longer timescale. The discourse timescale corresponds to repeated interactions with the same person; a series of pragmatic interactions. The third is the {\it developmental timescale}. The developmental timescale corresponds to the lifetime of an individual. It is composed of many interactions (on the pragmatic timescale), some of which with the same people (on the discourse timescale).  Many people interacting over their lifetimes lead to change at the {\it cultural timescale}. This is the timescale over which significant changes in language structure  occur (sometimes referred to as the ``language change" timescale). Finally, at the longest timescale, is the {\it evolution timescale}. All of the dynamics at lower timescales occur within a small slice in evolutionary time. While I will not have much to say about this timescale, its main significance is to situate the present claims with respect to claims about the innateness of language. Following \citeA{christiansen2008}, the suggestion is that there are aspects of language that are innate and constrain the dynamics of shorter timescales. Importantly, however, there are also  dynamics that take place at  shorter timescales, and these dynamics are the focus of the present section.

The phenomenon to be explained is why dynamics at the pragmatic timescale are reflected in structure at the cultural timescale. The proposal is that there are dynamics between adjacent timescales, and that, over time, these dynamics lead to change on longer timescales. Importantly, the character and phenomena of the dynamics between each pair of timescales are different. For example,  the dynamics between discourse and developmental timescales are reflected in cognitive changes in the mind of a particular speaker. In contrast, the dynamics between cultural and evolutionary timescales are reflected in genetic changes in linguistic abilities.

The idea of a causal relationship between language use and structure is not new. One of the earliest proposals of this idea was \citeA{whorf1956language} who argued that habitual patterns of talking in particular ways (what he called ``fashions of speaking") lead over time to different conceptualizations of the world.\footnote{This is an important nuance to claims about linguistic relativity that is often over-looked: It is not {\it that} a language has a label for a concept that matters,  but rather the presence of that label in conjunction with a developmental history of using that label.} Grammar is a case where this view as been particularly well articulated, under the heading of {\it Emergent Grammar} \cite{hopper1987emergent}: 

\begin{quote} The notion of Emergent Grammar is meant to suggest that structure, or regularity, comes out of discourse and is shaped by discourse as much as it shapes discourse in an on-going process. [...] Structure, then, in this view is not an overarching set of abstract principles, but more a question of a spreading of systematicity from individual words, phrases and small sets. (p. 142)
\end{quote}
More recently, cognitive psychologists has begun to formally model these dynamics. In this tradition, \citeA{bybee2005alternatives} write: ``Properties of formal structure [...] are facts about the structure that are to be explained as arising from the cumulative impact of the processes that shape each language, as it adapts through the process of language use" (p. 406). They argue for the value of a connectionist framework in capturing these dynamics. Also within a  connectionist framework, \citeA{mcmurray2012} highlight the relevance of different timescales in capturing the phenomenon of children's word learning across the developmental timescale. Perhaps the broadest framing of these dynamics has been by \citeA{christiansen2008}, who put propose a mechanism closely aligned with the present argument.

The goal of the present section is to synthesize these many claims about cumulative dynamics into a single framework. In what follows, I describe cognitive phenomena related to the dynamics between each pair of adjacent timescales, beginning with the two shortest timescales: pragmatics and discourse. 

%%% Prag and discourse %% 
\subsection{Dynamics between pragmatic and discourse timescales} 
The pragmatic timescale is the locus of language use (``one-shot" communication problems), but regularities emerge when language use is aggregated across multiple interaction with the same speaker. This is the discourse timescale. The dynamics between these  timescales are critical to understanding how interlocutors solve the problem of multiple equilibria. Recall coordination problems like the one described in the payoff matrix in Table 3: Speakers must figure out how to map two novel words onto two novel objects. The critical feature of this coordination problem is that there are two equilibrium points. The question then is, how do speakers mutually arrive at the same point? \citeA{schelling1980strategy} argues that speakers may move to equilibria that are more salient, either perceptually or given prior knowledge. For example, if one of the objects is flashing lights and beeping  loudly, while the other is a  piece of wood, there might be a perceptual bias to assume that the first word uttered corresponds to the flashing, beeping object.  However, if you are helping your partner build a shelf, there might be a bias to select the piece of wood, which is more salient given your knowledge of the speaker. Indeed, there is evidence in the psychological literature that interlocutors make use of salience in solving coordination problems \cite{clark1983common}. 

But how do speakers solve the the problem when there is no asymmetry between equilibria? \citeA{lewis1969convention} proposes the notion of {\it convention} to answer this question. He suggests that once speakers happen to successfully coordinate their behavior at a particular equilibrium, there is inertia to maintain that equilibrium rather than switch to an alternative which is, a priori, equally good. A series of  interactions over the pragmatic timescale thus lead to a convention at the discourse timescale. In the case of this example, the idea is that neither ``dax" nor ``fep" is a better name for Object A. But, once the speakers successfully coordinate by using ``dax" to refer to Object A, there is a pressure for both to continue using this linguistic form to refer to Object A. The speakers have thus established a convention and the more this convention is used, the more  it becomes entrenched. Knowledge of partner-specific conventions is one aspect of what \citeA{clark1996using} refers to as {\it common ground}.
% focal points

In the social psychology literature, there is a large body of work that speaks to the psychological processes that support the emergence of conventions. This work is under the rubric of ``conformity," where the idea is that individuals in social groups are motivated to conform to the perceived social norm  \cite{cialdini2004social}. When language use is couched as a particular case of social interaction, this work becomes relevant. The idea is that all mappings between linguistic form and meaning are arbitrary, but individuals are motivated to conform. Thus, once an equilibrium is established, speakers conform to the perceived norm (that ``dax" maps to Object A, for example).  Sherif's autokinetic experiment (1935)\nocite{sherif1935} provides a powerful demonstration of this pressure. In his task, participants viewed a dot of light on a wall and were asked to indicate when the dot moved and then estimate its distance. In reality, the light never moved. Nonetheless, all individuals reported seeing some movement in the light. Critically, in one version of the study, three strangers  were tested individually in the task. They were then tested as a group three additional times. The group context was identical to the individual context except that the subjects could overhear other subjects' responses. Figure 5 plots the median distance judgments of one group of three subjects over iterations of the experiment. When tested individually, the three subjects were highly variable in their distance judgments. However, when tested together, their judgments tended to converge, and this convergence increased over iterations of the experiment. This result provides a clear demonstration of the minimal conditions necessary for interacting social partners to converge on an arbitrary equilibrium. 

\begin{figure}
\begin{center} 
\includegraphics[width=3in]{figs/coordinatingmeaning.png}
\caption{\label{fig:results}  Plot reproduced from Sherif (1935), showing the median distance judgment (in inches) for three different subjects. The {\it x}-axis corresponds to four different iterations of the experiment: first individually, then three iterations as a group. Over time, the participants converge on an arbitrary norm.}
\end{center} 
\end{figure}

In the domain of language, this same phenomenon is observed in the way partners establish reference. Because there are multiple ways of referring to an object, interlocutors must establish some convention. This happens over the discourse timescale. \citeA{clark1986referring} demonstrate this phenomenon in the laboratory. In their task, pairs of naive subjects were randomly assigned to either the role of director or matcher. They were seated across from each other with an opaque wall  between them. Each subject had a set of cards with ambiguous images that was identical to their partner's. The director's cards were arranged in a grid, and her task was to direct the matcher to organize her cards in the same way using only verbal instruction. After repeatedly completing this task with the same partner, the directors began to use overall fewer words to describe the cards. For example, in trial one, one director used the phrase ``the next one looks like a person who's ice skating, except they're sticking their arms out in front," but by trial six the same director simply used the phrase ``the ice skater" to refer to that same card. This suggests that the interlocutors arrive at an equilibrium point about how to refer to the different referents, known as {\it lexical entrainment}. \citeA{brennan1996conceptual} argue that this shared way of referring to an object reflects a {\it conceptual pact} between interlocutors about how to conceptualize the referents. Consistent with this view, \citeA{metzing2003conceptual} show that this phenomenon is partner-specific, suggesting that low level cognitive effects (e.g.\ memory recency) cannot alone account for the observed coordination. Together, this line of work demonstrates how in-the-moment pragmatic pressures lead to equilibrium in discourse.

In addition to reference, there are a number of findings that suggest that interlocutors coordinate other aspects of language.  For example, in a study by \citeA{branigan2000syntactic}, interlocutors were found to coordinate their syntactic structures. The task involved completing a picture description task with a confederate in which the two alternated speaking. Critically,  sometimes the confederate used a prepositional object structure (e.g.\ ``The girl is throwing the ball to the dog.") and sometimes she used a double-object construction (e.g.\ ``The girl is throwing the dog the ball.").  Subjects tended to use the syntactic structure used by the confederate to describe their own picture (even though there was no semantic overlap between the two), suggesting a coordination of syntactic structure. Other work suggests low level perceptual coordination. For example,  \citeA{trude2012talker} find that speakers acquire speaker-specific knowledge about how individuals pronounce different words. While the cognitive mechanisms supporting these different types of coordination may differ,  each is a case of discourse-level coordination in language use. 

%%% Discourse and development %% 
\subsection{Dynamics between discourse and developmental timescales} 
Many repeated interactions on the pragmatic and discourse timescales lead to change on the developmental timescale. One way to think about this change is as {\it learning}. This learning is a sort of ``cached equilibrium" that results from aggregating over interactions with social partners that each arrives at a similar equilibrium. For example, in the case of semantics, learners make many inferences about the meaning of particular words in many particular interactions across time. The intended referent in each of these contexts is the result of an equilibrium in a coordination problem. Over time, the learner need not reason through the pragmatic logic for each context (e.g., ``If the speaker had meant object A, should would have said `dax' but she didn't, and so...", etc.). Rather she can make use of stored knowledge: speakers tend to use a particular label to refer to things that  have roughly these features. The induction of which features map on to a particular label is the process of inducing the semantic structures of language, like those described by \citeA{kemp2012kinship}.

A large body of work documents learners' ability to  use of pragmatic information to learn the meaning of new words \cite<e.g.>{baldwin1991infants, clark1987principle, xu2007, frank2014inferring}, although the developmental trajectory of these skills is not well-agreed upon. For example, \citeA{frank2014inferring} used a task analogous to \citeA{frank2012predicting}, but with novel words. To illustrate their task, imagine someone used the word ``dax" rather than ``circle" to refer to the middle object in Figure 1. What would you think ``dax" meant? As in the familiar word task, the prediction is that learners will assume that speakers are being informative by picking out the smallest set of features that are true of the referent, given the contextual alternatives (in this case, the concept \textsc{circle}). Preschool age children were found to reason in exactly this way, using the notion of informativeness to guide their inferences about word meaning. 

\begin{figure}
\begin{center} 
\includegraphics[width=5.5in]{figs/ME.png}
\caption{Plots reproduced from (a) Bion et al.\ (2012) and McMurray et al.\ (2012). In (a), 24-month-olds show a disassociation between their ability to map a novel word onto a novel object  (triangles) and retain that mapping when tested later on (squares).  Plot (b) presents a simulation of this result from an associative model. The model is able to successfully map a novel word onto a novel object (squares) before it succeeds in retaining that mapping over time (circles).}
\end{center} 
\end{figure}

But this in-the-moment inference is not itself learning. Learning, rather, is the result of storing information about these individuals interactions  and doing some sort of generalization across them. There is reason to think that  in-the-moment inference and learning are two distinct cognitive processes. For example, \citeA{bion2012fast} tested 24- and 30-month-olds in a task that required children to infer the referent of a novel word in the presence of a familiar and a novel object \cite<often referred to as the ``mutual exclusivity" task;>{markman1988, markman2003}.  24-month-olds were able to correctly infer the referent of the word in this context (the novel object), but showed no evidence of remembering this mapping when tested later (Fig. 6a). 30-month-olds, in contrast, both inferred the correct novel word and retained it over a short interval. The fact that these two skills are not coupled in  development suggests that they may rely on cognitively distinct processes.

Recent work by \citeA{mcmurray2012} captures these two distinct timescales in a computational framework. They demonstrate how a wide variety of word learning phenomena can arise from the   dynamics of an associative model.  They instantiate the role of the in-the-moment pragmatic timescale in terms of the activation of word and object nodes, and the effect of long-term learning in the associative weights on the links between nodes. Of particular note, they are able to capture the empirical pattern observed by  \citeA[Fig. 6b]{bion2012fast}. In one simulation, they tested a model which knew the meaning of some words but not others. The model was then tested in a setup similar to the \citeA{bion2012fast} task. In the inference task, the model was presented with two familiar objects, one novel object and a novel word and, with enough training, was able to infer that the novel word mapped to the novel object. A second setup tested retention, in which the model was tested with two novel objects, one familiar object and a novel word. These trials revealed that the model eventually retained the mapping that was made during the inference trials. Critically, however, the rates of the emergence of these patterns differ: the model quickly began to show in-the-moment inference, but only eventually began showing evidence for retention of these mappings. This demonstrates how the discrepant pattern observed by \citeA{bion2012fast} can emerge from the dynamics of a relatively simple associative model.

In related work, we have tried to understand the psychological forces supporting the bias to map a novel word  to a novel object \cite{lewis2013b}. There are two broad proposals for explaining this effect in the literature. One proposal is that children  rely on pragmatic reasoning \cite<``Why would you have used that weird word to refer to the familiar object, if you had intended the familiar object?;">{clark1987principle, diesendruck2001}. An alternative proposal is that children have a constraint on the types of lexicons they consider when learning the meaning of a new word --- namely, only those lexicons that have a one-to-one mapping between words and objects \cite{markman1988, markman2003}. One way to think about these different proposals is by the timescales over which they operate. A pragmatic constraint is a bias that relies on information available at the pragmatic or discourse timescale, while a one-to-one constraint is a bias that could be learned through experience over the developmental timescale. Using a hierarchical Bayesian model, we instantiate the pragmatic account through basic probabilistic properties of the model (this corresponds to they intuition: ``If the novel word mapped to the familiar object, that would make it really unlikely I had heard a different label for that familiar object so many times before!"). We instantiate the lexical  account by constraining the set of lexicons the learner considers. We show that, in principle, both sources of information, at two different timescales, pull in the same direction and lead the learner to select the novel object. This highlights an empirical challenge in trying to disentangle the relative contributions of information at each timescale to this inference.

A key component of learning is {\it generalization}: aggregating across tokens of examples observed over time in order to make predictions in new contexts. In acquiring language, children learn to generalize both the form and meaning of language. The are a number of forms these generalization could take. One possibility is that the form is simply a frequency count. For example, in the case of word learning, the child might track the number of times a word and object co-occur in the environment and then infer that the meaning of the word is the object that the word most often co-occurs with \cite<what is often referred to as ``cross-situational learning;">{pinker1984language, yu2007rapid,smith2008infants}.\footnote{Note that there is another generalization problem embedded in this problem: how to categorize objects as the same across contexts \cite{lewis2013a}. For example, this is the problem of recognizing that a dalmatian and a terrier are both instances of the same category \textsc{dog}. This problem must be solved jointly with the mapping problem.} This is slightly different than what is typically meant by ``generalization," but it has the critical character of aggregating across tokens in order to make a prediction in a new context (e.g. that ``dog" maps to \textsc{dog}). 

A second possible form of this generalization is an abstract {\it overhypothesis}. Overhypotheses have been called many things in the literature (e.g., theories, rules, schemas), but the key feature of this form of representation is that it is abstracted away from any particular observation. An example of such an overhypothesis is the understanding that shape is often the organizing feature of early  word meanings \cite{smith2002, kemp2007}. There is evidence that children as young as 9-months old can learn very simple overhypotheses \cite{dewar2010}. In understanding the dynamics of this abstraction over time, hierarchical Bayesian models have provided a powerful framework for thinking about these abstractions. The key insight from these models is that the induction of an abstraction happens with only very few instances, and that  high-level abstractions may be learned more quickly than lower-level abstractions \cite{goodman2011learning}. This provides a promising suggestion for how children might learn the regularities of a language from impoverished input. 

Regardless of the form that the generalization takes, the connection between the discourse and developmental timescales lies in the aggregation of instances of language use across time. In aggregating across instances, learners form some sort of generalization that allows them to make predictions in new contexts. This gives rise to two different cognitive processes: a discourse-based process in the experience of particular tokens of language use and a developmental-based process in the generalization of these tokens across time.

%%% Development and language %% 
\subsection{Dynamics between developmental and cultural timescales}

Generations of speakers acquiring and using language lead to the emergence of structure on the cultural timescale. These are the structural regularities discussed in Part I. This claim is the core of the argument presented by \citeA{christiansen2008}, which is summarized in Figure 7. The idea is that linguistic regularities emerge from groups of speakers playing linguistic coordination games over time. These regularities emerge in part because all speakers have the same pragmatic biases (as outlined by \citeA{horn1984}). Each generation of speakers produces their own set of structural regularities (or, what can simply be referred to as ``language"). Language change happens in the course of new speakers acquiring the language. This change is in the form of increasing regularity in the language. This more regularized language then becomes the input to the next generation of speakers. Thus, language, as a set of structural regularities,  is never a static entity, but rather a constantly evolving entity over the cultural timescale. 

\begin{figure}
\begin{center} 
\includegraphics[width=6in]{figs/timescales2}
\caption{Relationship between the developmental and cultural timescales. The proposal is that each generation of speakers, $P_n$, develops structural regularities that define a language, $L_n$. This language then becomes the input for the next generation of speakers, $P_n+1$. This iterative process leads to regularization in $L$ over the cultural timescale. }
\end{center} 
\end{figure}


There is evidence for this process of change in the laboratory. These experiments explore the dynamics of language change through iterated learning experiments. Each participant in these experiments is trained and tested on an aspect of an artificial language. The key feature of these experiments is that the testing output of $n$th participant is used as the training input for the $(n+1)$th participant. \citeA{kirby2008cumulative} provide an elegant demonstration of iterated learning in the laboratory. In their study, participants were presented with a novel language and asked to learn the pairings between words and novel images. For the first subject, the mappings between words and images were randomly generated. In the training phase, the subject was presented with an image and a label for that image. In the testing phase, they were shown a new image and asked to guess the word's meaning. This subject's responses were recorded, then divided into half (one half for training and one half for testing), and presented to a subsequent subject. The results revealed that as the number of generations increased, the language became more systematic in its mapping between features of the stimuli (e.g.\ color, shape, etc.) and syllables in the novel words. This provides evidence for the emergence of regularity over the cultural timescale.

The central issue in this theory is how and why regularity in structure emerges over the cultural timescale. The argument goes as follows. Language learners can only ever observe a subset of the language, and this creates a ``bottleneck" in the transmission process \cite{kirby2007evolution}. Consequently, a language can only be acquired if it can be learned through impoverished data. Critically, the only way to learn a whole language from limited input is through generalization. Put another way, the only way that a language can be transmitted through a bottleneck is through compression into generalized rules. Thus, in the course of their acquisition, learners change the messy input they receive to become more  regular. This has the consequence of making the language  easier to learn for subsequent generations. Through simulations, \citeA{brighton2005language} demonstrate that the size of the bottleneck is directly related to the degree of generalizations: when learners are given more input, they generalize less.

There is growing coherence in the empirical data around this view. There is evidence from several  natural contexts that suggests that children are biased to regularize their linguistic input \cite<e.g.,>{goldin1983gestural, senghas2001children}. However, this result stands in contrast with work with adults. A number of studies suggest that in artificial learning experiments, adults reproduce unpredictability in their input, rather than regularize it \cite<e.g.,>{hudson2005regularizing}. Why would we expect adults to behave differently than children? Recent computational work suggests a resolution to this inconsistency. \citeA{reali2009evolution} find that the regularity  that emerges over generations ultimately reflects the distribution of prior hypotheses of individual language learners. Critically, how fast the language converges to this regularity over generations depends on the strength of the prior. Thus, one way to think about the empirical difference between children and adults in these language learning experiments is in terms of their distribution over priors: children have weaker priors. However, with enough transmission of the language across generations, all languages should converge on the prior. Indeed this is what they find in an iterated learning experiment. While individual adult learners  match the variability in their input in a single generation, iterations of learning eventually leads to regularization.

%* note about learnability
%wever: real and griifiths, wonnacot and smith

%Finally, there is evidence for a systematic relationship between usage 

%In natural language, there is correlational evidence that language usage influences language change.  martin nowak,  michelle, liberman, pinker (nature, 2007) -  relationhip between rules and frequency of irr (quantifiying the evolutionary dynamics of language)
%It is in open question whether or not development plays an important role in the causal process between usage and the cultural timescale, but these data nonetheless suggest that whether, directly or indirectly, a pragmatic timescale phenomenon influences dynamics on the cultural timescale
%related to zipf correlational evidence  - law of abbreviation (regularity law?)
%zipf

 %\subsection{An empirical challenge: Sufficient but not necessary mechanisms}
%In what follows I describe two particularly challenging problems. Both are of the same general form: Two non-mutually exclusive causes are sufficient to give rise to an observable effect. The question is: To what degree does each of these causes contribute to the observed phenomenon?

%The first instance of this problem is the case of the one-to-one bias discussed in Part I.  In this case, the observable phenomenon is a bias for children to map a novel word onto a novel object  \cite<often referred to as ``mutual exclusivity" phenomenon>{markman1988, markman2003}. There are two plausible cognitive mechanism that could support this behavior. This first is pragmatic a the pragmatic \cite{clark1987, clark1988}  In \cite{lewis2013b}, we show in a computational framework that both of these mechanism could in principle {\it both} contribute to the observed behavior. This creates an empirical challenge

%Parallels between development and language structure, and development and pragmatics in the moment of language use - ontological status of regularities just because they exist doesn't' mean we represent them. ontological status of these regularities -- difference between this and other laws of nature (e.g. physics) is that we can represent them -- general abstract rules (connect to symbol literature). The question is: do we represent these regularities explicitly? (unlike macroeconomics) Just because we behave in away that is constant with macro economics, doesn't mean we = represent the regularity.

%The second instance of this problem is in understanding the source of the emergence of regularity in grammar over the cultural 

%to 

%\cite{posner and keele 1968} -- specific and general information used

%This issue is particular clear in the case of the one-to-one equilibrium in the structure of the lexicon. A one-to-one I
%* convergence on prior

\section{Conclusion} 
Language is an incredibly complex phenomenon, as evidenced by the wide range of perspectives on how and what to study. In the present paper, I have tried to suggest a unifying theory for thinking about language use and language structure. I have suggested a causal story for how  equilibria at the level of language use can lead to systematicity at the level of linguistic structure --- and, in particular, systematicity that reflects the equilibria at the pragmatic timescale. By starting with Horn's pragmatic framework, I  motivated a wide range of pragmatic phenomena. The key claim is that local dynamics between adjacent timescales lead to the emergence of structure at the cultural timescale. While my focus has been the role of pragmatics, it is important to note that  the claim is not a reduction of all linguistic phenomena to pragmatic principles. Rather, other factors like cognitive limitations are also likely to be important factors in the complete causal story.

The heart of the proposed theory is the micro-level processes that occur within the individual cognitive system. Understanding these processes, and how they lead to phenomena at the pragmatic, discourse and developmental timescales, is essential if we are to develop a complete understanding of language at every level. But, to know how these cognitive processes are related to broader linguistic phenomena, we must have a theory of how these phenomena are related to each other. That is the goal of the present paper. While it may be convenient and scientifically necessary to limit our focus of study to particular aspects of language, a broader theory of how these aspects are related  to each other is important for guiding our  enquiry. 

However, testing the predictions of this theory is not an easy empirical task. The proposal is a long and complex causal chain of events leading to the emergence of linguistic structure. A large part of the empirical challenge arises from the fact that the target phenomena unfold  across time, across varying timescales, and because the phenomena cannot be observed in any individual alone. Furthermore, in cases where we do have evidence for aspects of this causal chain, it is only for the dynamics between one or two timescales. For this reason, computational models will provide an invaluable tool in future work for making testable predictions from this complex causal theory. 

%diachronic perspective"
%What this is not -- totally reductionist to pragmatis. As christiansen and CHater argue, multiple %factors (most notable things like memory).
%What is the alternative theory -- all theories of language echange must make reference to individual. ?
%HBB's as tool
%- the heart of this is the cognitive processes at the most micro level of this cognitive process

% TO DO:
%- proof everything
%- fix tables 
%- fix preview paragraphs in intro
% write language change paragraph
\newpage
\bibliographystyle{apacite}
\bibliography{biblibrary}

\end{document}

